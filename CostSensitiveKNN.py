"""
CostSensitiveKNN Algorithm
"""
from KNN import KNN
# from utils import Examples, TRAIN_PATH, TEST_PATH, CommitteeWrapper, insort, euclidean_distance // TODO: put it again
from utils import *  # TODO: Delete
from ID3 import ID3ContinuousFeatures

BEST_LOSS_K = 1
BEST_BOUND = 0.056

""""""""""""""""""""""""""""""""""""""""""" CostSensitiveKNN """""""""""""""""""""""""""""""""""""""""""


class CostSensitiveKNN(KNN):
    def __init__(self, train_path: str, bound: float = BEST_BOUND, k: int = BEST_LOSS_K):
        super().__init__(train_path, k)
        self._bound = bound
        self._id3_classifier = None

    def classify(self, test_path: str) -> float:
        return self.classify_and_get_loss(test_path)

    def classify_one(self, test_example: Examples) -> int:
        committee = []
        for train_example in self._train_examples:
            insort(committee, CommitteeWrapper(train_example[0], euclidean_distance(test_example, train_example)))

        votes_num, vote_for, vote_against, under_bound = 0, 0, 0, (0, 0)
        for vote in committee:
            if vote.distance < self._bound:
                under_bound = (vote_for, vote_against)
            if votes_num >= self._k:
                break
            if vote == 1:
                vote_for += 1
            else:
                vote_against += 1
            votes_num += 1

        if vote_for >= vote_against:
            return 1

        # KNN wants to classify as Negative to disease (healthy), let's get a second opinion
        if self._id3_classifier is None:
            self._id3_classifier = ID3ContinuousFeatures(self._train_examples)

        if self._id3_classifier.classify_one(test_example):
            return 1

        # Both KNN and ID3 want to classify as Negative to disease (healthy), let's get a third final opinion
        return under_bound[0] > under_bound[1]

    ######### Helper Functions for CostSensitiveKNN Algorithm #########

    def _get_accuracy(self, test_examples: Examples) -> float:
        return self._get_loss(test_examples)


""""""""""""""""""""""""""""""""""""""""""" Main """""""""""""""""""""""""""""""""""""""""""


def experiment_loop(train_path):
    train_examples = my(train_path)
    folds = KFold(n_splits=N_SPLIT, shuffle=SHUFFLE, random_state=RANDOM_STATE)
    k_values = [i for i in range(1, 16)]
    k_accuracy = []

    for k in k_values:
        for bound in (i/100 for i in range(10)):
            accuracy = 0
            for train_fold, test_fold in folds.split(train_examples):
                accuracy += CostSensitiveKNN(np.take(train_examples, train_fold, 0), bound, k).classify(
                    np.take(train_examples, test_fold, 0))
            k_accuracy.append((k, bound, accuracy / N_SPLIT))
            print(f'finished check k={k}, bound={bound}, got:\n{k_accuracy}\n')

    return k_accuracy


def experiment_random(train_path):
    train_examples = my(train_path)
    folds = KFold(n_splits=N_SPLIT, shuffle=SHUFFLE, random_state=RANDOM_STATE)
    k_accuracy=[(1, 0.056, 0.01865302642796249), (6, 0.54, 0.02329923273657289), (9, 0.14, 0.012774936061381076),
     (15, 0.06, 0.016001705029838022), (5, 0.42, 0.03095907928388747), (9, 0.49, 0.02184569479965899),
     (2, 0.76, 0.016875532821824378), (12, 0.54, 0.02566069906223359), (15, 0.471, 0.024782608695652176),
     (7, 0.242, 0.013358908780903664), (15, 0.175, 0.016001705029838022), (6, 0.96, 0.02329923273657289),
     (4, 0.083, 0.016585677749360615), (12, 0.984, 0.02566069906223359), (6, 0.327, 0.014518329070758737),
     (12, 0.516, 0.025950554134697358), (14, 0.838, 0.025362318840579705), (15, 0.036, 0.016001705029838022),
     (4, 0.375, 0.016585677749360615), (12, 0.415, 0.02566069906223359), (3, 0.959, 0.01952685421994885),
     (9, 0.686, 0.021845694799658994), (12, 0.019, 0.016879795396419435), (11, 0.308, 0.016005967604433076),
     (1, 0.747, 0.01865302642796249), (8, 0.839, 0.022433930093776637), (5, 0.074, 0.022178175618073316),
     (9, 0.095, 0.012774936061381076), (8, 0.014, 0.013653026427962489), (10, 0.088, 0.01335464620630861),
     (12, 0.008, 0.016879795396419435), (7, 0.02, 0.013358908780903664), (6, 0.076, 0.014518329070758737),
     (8, 0.041, 0.013653026427962489), (13, 0.034, 0.016001705029838022), (5, 0.1, 0.022178175618073316),
     (5, 0.024, 0.022178175618073316), (7, 0.069, 0.013358908780903664), (1, 0.055, 0.01865302642796249),
     (13, 0.066, 0.016001705029838022), (7, 0.042, 0.013358908780903664), (9, 0.023, 0.012774936061381076),
     (2, 0.095, 0.016875532821824378), (13, 0.097, 0.016001705029838022), (8, 0.069, 0.013653026427962489),
     (1, 0.041, 0.01865302642796249), (14, 0.095, 0.01658141517476556), (2, 0.061, 0.016875532821824378),
     (7, 0.087, 0.013358908780903664), (10, 0.045, 0.01335464620630861), (5, 0.037, 0.022178175618073316),
     (15, 0.036, 0.016001705029838022), (14, 0.1, 0.01658141517476556), (1, 0.017, 0.01865302642796249),
     (10, 0.06, 0.01335464620630861), (6, 0.038, 0.014518329070758737), (6, 0.061, 0.014518329070758737),
     (13, 0.064, 0.016001705029838022), (2, 0.052, 0.016875532821824378), (14, 0.025, 0.01658141517476556),
     (14, 0.06, 0.01658141517476556), (3, 0.017, 0.01952685421994885), (11, 0.078, 0.016005967604433076),
     (3, 0.049, 0.01952685421994885), (13, 0.077, 0.016001705029838022), (7, 0.007, 0.013358908780903664),
     (5, 0.091, 0.022178175618073316), (13, 0.035, 0.016001705029838022), (1, 0.024, 0.01865302642796249),
     (15, 0.01, 0.016001705029838022), (8, 0.026, 0.013653026427962489), (12, 0.1, 0.016879795396419435),
     (7, 0.091, 0.013358908780903664), (2, 0.012, 0.016875532821824378), (12, 0.081, 0.016879795396419435),
     (4, 0.024, 0.016585677749360615), (10, 0.032, 0.01335464620630861), (12, 0.063, 0.016879795396419435),
     (6, 0.049, 0.014518329070758737), (8, 0.095, 0.013653026427962489), (15, 0.008, 0.016001705029838022),
     (9, 0.042, 0.012774936061381076), (15, 0.046, 0.016001705029838022), (15, 0.049, 0.016001705029838022),
     (6, 0.029, 0.014518329070758737), (5, 0.077, 0.022178175618073316), (12, 0.081, 0.016879795396419435),
     (3, 0.037, 0.01952685421994885), (14, 0.094, 0.01658141517476556), (13, 0.055, 0.016001705029838022),
     (9, 0.077, 0.012774936061381076), (2, 0.072, 0.016875532821824378), (7, 0.054, 0.013358908780903664),
     (7, 0.052, 0.013358908780903664), (3, 0.041, 0.01952685421994885), (3, 0.044, 0.01952685421994885),
     (9, 0.063, 0.012774936061381076), (1, 0.042, 0.01865302642796249), (13, 0.032, 0.016001705029838022),
     (6, 0.046, 0.014518329070758737), (8, 0.093, 0.013653026427962489), (12, 0.03, 0.016879795396419435),
     (13, 0.096, 0.016001705029838022), (10, 0.09, 0.01335464620630861), (7, 0.039, 0.013358908780903664),
     (6, 0.044, 0.014518329070758737), (11, 0.004, 0.016005967604433076), (15, 0.07, 0.016001705029838022),
     (12, 0.091, 0.016879795396419435), (9, 0.026, 0.012774936061381076), (14, 0.074, 0.01658141517476556),
     (11, 0.045, 0.016005967604433076), (8, 0.098, 0.013653026427962489), (8, 0.015, 0.013653026427962489),
     (8, 0.042, 0.013653026427962489), (5, 0.068, 0.022178175618073316), (9, 0.1, 0.012774936061381076),
     (6, 0.064, 0.014518329070758737), (1, 0.032, 0.01865302642796249), (3, 0.075, 0.01952685421994885),
     (8, 0.032, 0.013653026427962489), (8, 0.094, 0.013653026427962489), (5, 0.092, 0.022178175618073316),
     (15, 0.006, 0.016001705029838022), (10, 0.093, 0.01335464620630861), (11, 0.055, 0.016005967604433076),
     (2, 0.023, 0.016875532821824378), (3, 0.033, 0.01952685421994885), (3, 0.055, 0.01952685421994885),
     (11, 0.048, 0.016005967604433076), (11, 0.077, 0.016005967604433076), (9, 0.079, 0.012774936061381076),
     (4, 0.014, 0.016585677749360615), (14, 0.016, 0.01658141517476556), (13, 0.042, 0.016001705029838022),
     (13, 0.087, 0.016001705029838022), (4, 0.098, 0.016585677749360615), (8, 0.009, 0.013653026427962489),
     (14, 0.057, 0.01658141517476556), (9, 0.065, 0.012774936061381076), (9, 0.024, 0.012774936061381076),
     (6, 0.042, 0.014518329070758737), (7, 0.036, 0.013358908780903664), (9, 0.088, 0.012774936061381076),
     (6, 0.024, 0.014518329070758737), (14, 0.045, 0.01658141517476556), (2, 0.013, 0.016875532821824378),
     (2, 0.078, 0.016875532821824378), (4, 0.07, 0.016585677749360615), (8, 0.049, 0.013653026427962489),
     (4, 0.059, 0.016585677749360615), (9, 0.092, 0.012774936061381076), (15, 0.062, 0.016001705029838022),
     (10, 0.082, 0.01335464620630861), (4, 0.052, 0.016585677749360615), (10, 0.096, 0.01335464620630861),
     (1, 0.095, 0.01865302642796249), (2, 0.013, 0.016875532821824378), (1, 0.097, 0.01865302642796249),
     (3, 0.016, 0.01952685421994885), (2, 0.071, 0.016875532821824378), (10, 0.055, 0.01335464620630861),
     (14, 0.025, 0.01658141517476556), (11, 0.024, 0.016005967604433076), (11, 0.066, 0.016005967604433076),
     (6, 0.095, 0.014518329070758737), (8, 0.04, 0.013653026427962489), (4, 0.099, 0.016585677749360615),
     (3, 0.015, 0.01952685421994885), (8, 0.025, 0.013653026427962489), (3, 0.019, 0.01952685421994885),
     (14, 0.054, 0.01658141517476556), (11, 0.011, 0.016005967604433076), (12, 0.091, 0.016879795396419435),
     (14, 0.042, 0.01658141517476556), (2, 0.058, 0.016875532821824378), (10, 0.076, 0.01335464620630861),
     (9, 0.014, 0.012774936061381076), (15, 0.013, 0.016001705029838022), (3, 0.072, 0.01952685421994885),
     (4, 0.073, 0.016585677749360615), (7, 0.084, 0.013358908780903664), (13, 0.098, 0.016001705029838022),
     (14, 0.049, 0.01658141517476556), (1, 0.098, 0.01865302642796249), (6, 0.049, 0.014518329070758737),
     (5, 0.012, 0.022178175618073316), (8, 0.003, 0.013653026427962489), (3, 0.042, 0.01952685421994885),
     (7, 0.009, 0.013358908780903664), (1, 0.09, 0.01865302642796249), (14, 0.07, 0.01658141517476556),
     (11, 0.055, 0.016005967604433076), (9, 0.061, 0.012774936061381076), (6, 0.07, 0.014518329070758737),
     (2, 0.049, 0.016875532821824378), (14, 0.072, 0.01658141517476556), (12, 0.001, 0.016879795396419435),
     (4, 0.071, 0.016585677749360615), (4, 0.086, 0.016585677749360615), (11, 0.04, 0.016005967604433076),
     (1, 0.052, 0.01865302642796249), (8, 0.055, 0.013653026427962489), (12, 0.01, 0.016879795396419435),
     (6, 0.049, 0.014518329070758737), (1, 0.012, 0.01865302642796249), (3, 0.001, 0.01952685421994885),
     (4, 0.048, 0.016585677749360615), (3, 0.005, 0.01952685421994885), (15, 0.051, 0.016001705029838022),
     (2, 0.076, 0.016875532821824378), (9, 0.029, 0.012774936061381076), (4, 0.091, 0.016585677749360615),
     (10, 0.042, 0.01335464620630861), (6, 0.067, 0.014518329070758737), (1, 0.043, 0.01865302642796249),
     (13, 0.099, 0.016001705029838022), (15, 0.027, 0.016001705029838022), (2, 0.011, 0.016875532821824378),
     (7, 0.092, 0.013358908780903664), (2, 0.083, 0.016875532821824378), (3, 0.009, 0.01952685421994885),
     (1, 0.076, 0.01865302642796249), (10, 0.082, 0.01335464620630861), (8, 0.067, 0.013653026427962489),
     (14, 0.095, 0.01658141517476556), (3, 0.002, 0.01952685421994885), (13, 0.012, 0.016001705029838022),
     (9, 0.078, 0.012774936061381076), (10, 0.06, 0.01335464620630861), (13, 0.039, 0.016001705029838022),
     (9, 0.003, 0.012774936061381076), (6, 0.065, 0.014518329070758737), (9, 0.054, 0.012774936061381076),
     (12, 0.947, 0.02566069906223359), (10, 0.355, 0.02213554987212276), (5, 0.932, 0.03095907928388747),
     (1, 0.767, 0.01865302642796249), (15, 0.326, 0.016001705029838022), (7, 0.129, 0.013358908780903664),
     (15, 0.916, 0.024782608695652176), (1, 0.859, 0.01865302642796249), (9, 0.737, 0.021555839727195224),
     (15, 0.005, 0.016001705029838022), (8, 0.98, 0.022433930093776637), (14, 0.907, 0.025362318840579705),
     (1, 0.256, 0.01865302642796249), (12, 0.532, 0.025950554134697358), (1, 0.554, 0.01865302642796249),
     (6, 0.612, 0.02329923273657289)]

    while True:
        accuracy = 0
        k = randint(1, 15)
        bound = randint(1, 1000) / 1000
        for train_fold, test_fold in folds.split(train_examples):
            accuracy += CostSensitiveKNN(np.take(train_examples, train_fold, 0), bound, k).classify(
                np.take(train_examples, test_fold, 0))
        k_accuracy.append((k, bound, accuracy / N_SPLIT))
        print(f'finished check k={k}, bound={bound}, got:\n{k_accuracy}\n')


def main():
    print(CostSensitiveKNN(TRAIN_PATH).classify(TEST_PATH))


if __name__ == "__main__":
    # main()
    print(experiment_random(TRAIN_PATH))